---
title: "Project_2"
author: "Fabio_Savorgnan"
date: "3/14/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(message = F)
```


```{r }
library(dplyr)
library(ggplot2)
library(tidyr)
library(SDMTools)
library(readr)
library(digest)
library(ISLR)
library(car) 
library(leaps)
library( Matrix)
library(foreach)
library(glmnet)
library(gridExtra)
library(lsmeans)
library(limma)
library(Sleuth3)
library(tseries)
library(forecast)
library(ggplot2)
library(MASS)
library(mvtnorm)
library(epitools)
library(samplesizeCMH)
library(caret)
library(GGally)
library(glmnet)
library(bestglm)
library(data.table)
library(broom)
library(plyr)
library(repr)
library(ResourceSelection)
library(ROCR)
library(pROC)
```

# Loading the data
```{r }
bank_20 = read.csv("bank-additional-full.csv", sep=";")
head(bank_20)
```


# Reshape for a balance data
```{r }
clean_bank_20<-bank_20
yes_indices = which(clean_bank_20$y == "yes")
yes_train_indices = sample(yes_indices, length(yes_indices) * .9)
no_indices = which(clean_bank_20$y == "no")
no_train_indices = sample(no_indices, length(yes_indices))
train_indices = c(no_train_indices,yes_train_indices)

balanced_train_bank_20 = clean_bank_20[train_indices,]
balanced_train_bank_20<-balanced_train_bank_20
head(balanced_train_bank_20)
test_bank_20 = clean_bank_20[-train_indices,]
test_bank_20<-test_bank_20
head(test_bank_20)

```




#PCA for continuous variables
```{r }
pc.bc<-prcomp(bank_20[,-c(2,3,4,5,6,7,8,9,10,15,21)],scale.=TRUE)
pc.bc.scores<-pc.bc$x
#Adding the response column to the PC's data frame
pc.bc.scores<-data.frame(pc.bc.scores)
pc.bc.scores$loan<-bank_20$y

#Use ggplot2 to plot the first few pc's
library(ggplot2)
ggplot(data = pc.bc.scores, aes(x = PC1, y = PC2)) +
  geom_point(aes(col= loan), size=1)+
  ggtitle("PCA of Credict Assessment")
```

# Heat map for continuous variables
```{r }
my.cor<-cor(bank_20[,-c(2,3,4,5,6,7,8,9,10,15,21)])
my.cor
library(gplots)
library(ggplot2)
heatmap.2(my.cor,col=redgreen(75), 
          density.info="none", trace="none", dendrogram=c("row"), 
          symm=F,symkey=T,symbreaks=T, scale="none")
```


# Pair correlation numerical variables
```{r }
# Prepare some data
df <- bank_20[,-c(2,3,4,5,6,7,8,9,10,15,21)]
# Correlation plot
ggcorr(df, palette = "RdBu", label = TRUE)
```


# Running LDA
```{r }
# Build X_train, y_train, X_test, y_test
X_train <- balanced_train_bank_20[,-c(2,3,4,5,6,7,8,9,10,15,21)]
y_train<-balanced_train_bank_20[,21]

X_test <- test_bank_20[,-c(2,3,4,5,6,7,8,9,10,15,21)]
y_test <- test_bank_20[,21]
mylda<-lda(y ~ age + duration + campaign+pdays + previous + emp.var.rate + cons.price.idx + cons.conf.idx + euribor3m + nr.employed, data= balanced_train_bank_20)
pred<-predict(mylda,newdata=X_test )$class  
Truth<-y_test
x<-table(pred,Truth) # Creating a confusion matrix
x
#Missclassification Error
ME<-(x[2,1]+x[1,2])/sum(x)
FP<-x[2,1]/(x[2,1]+x[1,1])
FN<-x[1,2]/(x[1,2]+x[2,2])
ses<-x[2,2]/(x[2,2]+x[1,2])
sp<-x[1,1]/(x[1,1]+x[2,1])
accuracy <- x
AC<- sum(diag(accuracy))/sum(accuracy)
TP= 1-FN
AC
TP
ME
FP
FN
ses
sp
#Calculating overall accuracy
oneminusME<-1-ME
oneminusME
```


# ROC for the LDA model
```{r }
X_train <- balanced_train_bank_20[,-c(2,3,4,5,6,7,8,9,10,15,21)]
y_train<-balanced_train_bank_20[,21]
y_train <- as.factor(as.character(y_train))
X_test <- test_bank_20[,-c(2,3,4,5,6,7,8,9,10,15,21)]
y_test <- test_bank_20[,21]
y_test <- as.factor(as.character(y_test))
mylda<-lda(y ~ age + duration + campaign+pdays + previous + emp.var.rate + cons.price.idx + cons.conf.idx + euribor3m + nr.employed, data= balanced_train_bank_20)
pred.lda<-predict(mylda,newdata=X_test )
predsld <- pred.lda$posterior
predsld <- as.data.frame(predsld)
predsld <- prediction(predsld[,2],y_test)
roc.perfld = performance(predsld, measure = "tpr", x.measure = "fpr")
auc.trainld <- performance(predsld, measure = "auc")
auc.trainld <- auc.trainld@y.values
plot(roc.perfld)
abline(a=0, b= 1)
text(x = .40, y = .6,paste("AUC = ", round(auc.trainld[[1]],3), sep = ""))
```



# Selection of the variable for LLR 

# Forward

```{r }
model.main<- glm(y~ ., balanced_train_bank_20,family = binomial(link="logit"))
model.null<-glm(y ~ 1, data=balanced_train_bank_20,family = binomial(link="logit"))
step(model.null,
     scope = list(upper=model.main),
     direction="forward",
     test="Chisq",
     data=balanced_train)
```

# Prediction with the forward selection

```{r }
logr<-glm(y ~ duration + job + contact + day_of_week + default + previous+ pdays, family = binomial(link = "logit"), data =  balanced_train_bank_20)

# 1 way
logr.probs<-predict(logr, newdata=test_bank_20)
logr.pred<-rep("No",32372)
logr.pred[logr.probs>.5]="Yes"
Truth<-test_bank_20[,21]
Pred<-logr.pred
ftable(addmargins(table(Pred,Truth)))
x<-table(Pred,Truth)
ME<-(x[1,2]+x[2,1])/sum(x)
FP<-x[2,1]/(x[2,1]+x[1,1])
FN<-x[1,2]/(x[1,2]+x[2,2])
ses<-x[2,2]/(x[2,2]+x[1,2])
sp<-x[1,1]/(x[1,1]+x[2,1])
accuracy <- x
AC<- sum(diag(accuracy))/sum(accuracy)
TP= 1-FN
AC
TP
ME
FP
FN
ses
sp
# 2 way
#pred = predict(logr, newdata=test_bank_20)
#accuracy <- table(pred, test_bank_20[,21])
#sum(diag(accuracy))/sum(accuracy)
#confusionMatrix(factor(pred, levels = 1:928), factor(balanced_test$y, levels = 1:928))

```

# Holmes test for forward selecttion
```{r }
hoslem.test(forward$y, fitted(forward), g=10)

```



# ROC curve for the forward model
```{r }
dat.train.x <- as.data.frame(balanced_train_bank_20)
dat.train.y <- balanced_train_bank_20$y
dat.train.y <- as.factor(as.character(dat.train.y))
dat.test.x<-as.data.frame(test_bank_20)
dat.test.y<- test_bank_20$y
dat.test.y <- as.factor(as.character(dat.test.y))

logr1<-glm(formula = y ~ duration + job + contact + day_of_week + default + previous+ pdays, family = binomial(link = "logit"), data = balanced_train_bank_20)
logr.probs1<-predict(logr1, newdata=dat.test.x)

#Compare the prediction to the real outcome
head(logr.probs1)
head(dat.train.y)

#Create ROC curves
pred1 <- prediction(logr.probs1, dat.test.y)
roc.perf1 = performance(pred1, measure = "tpr", x.measure = "fpr")
auc.train1 <- performance(pred1, measure = "auc")
auc.train1 <- auc.train1@y.values

#Plot ROC
plot(roc.perf1)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train1[[1]],3), sep = ""))

```




# Stepwise selection 
```{r }
full.model <- glm(y ~., data = balanced_train_bank_20, family = binomial)
coef(full.model)
step.model <- full.model %>% stepAIC(trace = FALSE)
coef(step.model)
# Make predictions
probabilities <- full.model %>% predict(test_bank_20, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "yes", "no")
# Prediction accuracy
observed.classes <- test_bank_20$y
mean(predicted.classes == observed.classes)
# Make predictions
probabilities <- predict(step.model, test_bank_20, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, "yes", "no")
# Prediction accuracy
observed.classes <- test_bank_20$y
mean(predicted.classes == observed.classes)

```


# Plot the stepwise model
```{r }
plot(step.model)
```


# Hoslem test for stepwise selection
```{r }
hoslem.test(step.model$y, fitted(step.model),g=10)
```

# VIF and odd ratio for stepwise selection
```{r }
exp(cbind("Odds ratio" = coef(step.model), confint.default(step.model, level = 0.95)))
vif(step.model)
```


# Prediction with the stepwise  selection after taking care of the high vif

```{r }
logr<- glm(y ~ job + default + contact + month + duration + campaign + pdays +poutcome, family = binomial(link = "logit"),data = balanced_train_bank_20)

# 1 way
logr.probs<-predict(logr, newdata=test_bank_20)
logr.pred<-rep("No",32372)
logr.pred[logr.probs>.5]="Yes"
Truth<-test_bank_20[,21]
Pred<-logr.pred
ftable(addmargins(table(Pred,Truth)))
x<-table(Pred,Truth)
ME<-(x[1,2]+x[2,1])/sum(x)
FP<-x[2,1]/(x[2,1]+x[1,1])
FN<-x[1,2]/(x[1,2]+x[2,2])
ses<-x[2,2]/(x[2,2]+x[1,2])
sp<-x[1,1]/(x[1,1]+x[2,1])
accuracy <- x
AC<- sum(diag(accuracy))/sum(accuracy)
TP= 1-FN
AC
TP
ME
FP
FN
ses
sp
# 2 way
#pred = predict(logr, newdata=test_bank_20)
#accuracy <- table(pred, test_bank_20[,21])
#sum(diag(accuracy))/sum(accuracy)
#confusionMatrix(factor(pred, levels = 1:928), factor(balanced_test$y, levels = 1:928))
```

# Step model more detail as the selected model
```{r }
stf<-logr<- glm(y ~ job + default + contact + month + duration + campaign + pdays +poutcome, family = binomial(link = "logit"),data = balanced_train_bank_20)
exp(cbind("Odds ratio" = coef(stf), confint.default(stf, level = 0.95)))
vif(stf)
summary(stf)
confint(stf)

```

# ROC curve for the stepwise model
```{r }
dat.train.x <- as.data.frame(balanced_train_bank_20)
dat.train.y <- balanced_train_bank_20$y
dat.train.y <- as.factor(as.character(dat.train.y))
dat.test.x<-as.data.frame(test_bank_20)
dat.test.y<- test_bank_20$y
dat.test.y <- as.factor(as.character(dat.test.y))

logr2<- glm(y ~ job + default + contact + month + duration + campaign + pdays +poutcome, family = binomial(link = "logit"),data = balanced_train_bank_20)
logr.probs2<-predict(logr2, newdata=dat.test.x)

#Compare the prediction to the real outcome
head(logr.probs2)
head(dat.train.y)

#Create ROC curves
pred2 <- prediction(logr.probs2, dat.test.y)
roc.perf2 = performance(pred2, measure = "tpr", x.measure = "fpr")
auc.train2 <- performance(pred2, measure = "auc")
auc.train2 <- auc.train2@y.values

#Plot ROC
plot(roc.perf2)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train2[[1]],3), sep = ""))

```
# Step model more issues
```{r }
hoslem.test(balanced_train_bank_20$y, fitted(stf),g=10)

```

# Backward selection
```{r }
model.null<-glm(y ~ ., data=balanced_train_bank_20,family = binomial(link="logit"))
step(model.null,
     scope = list(upper=model.main),
     direction="backward",
     test="Chisq",
     data=balanced_train)
```



# Prediction with the backward selection

```{r }
logr<- glm(y ~ job + education + default + contact +duration + previous + pdays + campaign, family = binomial(link = "logit"),data = balanced_train_bank_20)

# 1 way
logr.probs<-predict(logr, newdata=test_bank_20)
logr.pred<-rep("No",32372)
logr.pred[logr.probs>.5]="Yes"
Truth<-test_bank_20[,21]
Pred<-logr.pred
ftable(addmargins(table(Pred,Truth)))
x<-table(Pred,Truth)
ME<-(x[1,2]+x[2,1])/sum(x)
FP<-x[2,1]/(x[2,1]+x[1,1])
FN<-x[1,2]/(x[1,2]+x[2,2])
ses<-x[2,2]/(x[2,2]+x[1,2])
sp<-x[1,1]/(x[1,1]+x[2,1])
accuracy <- x
AC<- sum(diag(accuracy))/sum(accuracy)
TP= 1-FN
AC
TP
ME
FP
FN
ses
sp
# 2 way
#pred = predict(logr, newdata=test_bank_20)
#accuracy <- table(pred, test_bank_20[,21])
#sum(diag(accuracy))/sum(accuracy)
#confusionMatrix(factor(pred, levels = 1:928), factor(balanced_test$y, levels = 1:928))

```



# ROC curve for the backward model
```{r }
dat.train.x <- as.data.frame(balanced_train_bank_20)
dat.train.y <- balanced_train_bank_20$y
dat.train.y <- as.factor(as.character(dat.train.y))
dat.test.x<-as.data.frame(test_bank_20)
dat.test.y<- test_bank_20$y
dat.test.y <- as.factor(as.character(dat.test.y))

logr3<- glm(y ~ job + education + default + contact +duration + pdays + campaign + poutcome, family = binomial(link = "logit"),data = balanced_train_bank_20)
logr.probs3<-predict(logr3, newdata=test_bank_20)

#Compare the prediction to the real outcome
head(logr.probs3)
head(dat.train.y)

#Create ROC curves
pred3 <- prediction(logr.probs3, dat.test.y)
roc.perf3 = performance(pred3, measure = "tpr", x.measure = "fpr")
auc.train3 <- performance(pred3, measure = "auc")
auc.train3 <- auc.train3@y.values

#Plot ROC
plot(roc.perf3)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train3[[1]],3), sep = ""))

```


# Lasso Feature selection
```{r }
dat.train.x <- model.matrix(y~ .,balanced_train_bank_20)
dat.train.y<-balanced_train_bank_20[,21]
cvfit <- cv.glmnet(dat.train.x, dat.train.y, family = "binomial", type.measure = "class", nlambda = 1000)
plot(cvfit)
coef(cvfit, s = "lambda.min")
#CV misclassification error rate 
cvfit$cvm[which(cvfit$lambda==cvfit$lambda.min)]

#Optimal penalty
cvfit$lambda.min

```

# Prediction
```{r }
# Final
dat.test.y<- test_bank_20[,21]
dat.test.y<-ifelse(dat.test.y == "yes", 1,0)
dat.test.x<- model.matrix(y~ .,test_bank_20)
finalmodel<-glmnet(dat.train.x, dat.train.y, family = "binomial",lambda=cvfit$lambda.min)

preds <- predict(finalmodel, s = cvfit$lambda.min, newx = dat.test.x)
Mypreds<-ifelse(preds>.5, 1,0)

final <- cbind(dat.test.y, Mypreds)
testMSE_LASSO<-mean((dat.test.y-Mypreds)^2)
testMSE_LASSO

# Checking the first six obs
head(final)

# RSQ
actual <- dat.test.y
preds <- pred
rss <- sum((preds - actual) ^ 2)
tss <- sum((actual - mean(actual)) ^ 2)
rsq <- 1 - rss/tss
rsq

```



# Sens/spec for lasso model confussion matrix

```{r }
logr.pred<-preds
logr.pred<-rep("No",32372)
logr.pred[logr.probs>.5]="Yes"
Truth<-test_bank_20[,21]
Pred<-logr.pred
x<- ftable(addmargins(table(Pred,Truth)))
x<-table(Pred,Truth)
ME<-(x[1,2]+x[2,1])/sum(x)
FP<-x[2,1]/(x[2,1]+x[1,1])
FN<-x[1,2]/(x[1,2]+x[2,2])
ses<-x[2,2]/(x[2,2]+x[1,2])
sp<-x[1,1]/(x[1,1]+x[2,1])
accuracy <- x
AC<- sum(diag(accuracy))/sum(accuracy)
TP= 1-FN
AC
TP
ME
FP
FN
ses
sp
# 2 way
#pred = predict(logr, newdata=test_bank_20)
#accuracy <- table(pred, test_bank_20[,21])
#sum(diag(accuracy))/sum(accuracy)
#confusionMatrix(factor(pred, levels = 1:928), factor(balanced_test$y, levels = 1:928))

```



# ROC for the lasso selection model

```{r }
# Predict from model
preds <- predict(finalmodel, newx = dat.test.x, type = 'response')

# Calculate true positive rate and false positive rate on the prediction object
perfl <- performance(prediction(preds, dat.test.y), 'tpr', 'fpr')

auc.train <- performance(prediction(preds, dat.test.y), measure = "auc")
auc.train <- auc.train@y.values

#Plot ROC
plot(perfl,main="LASSO")
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.train[[1]],3), sep = ""))

```




# Compare ROC from differents model
```{r }
plot( roc.perf1, colorize = TRUE)
plot(roc.perf2, add = TRUE, colorize = TRUE)
plot(roc.perf3, add = TRUE, colorize = TRUE)
plot(perfl, add = TRUE, colorize = TRUE)
abline(a=0, b= 1)

#without color for cutoff; but adding colors to allow for comarisons of the curves
plot( roc.perf1, col="black", add = TRUE)
plot(roc.perf2,col="orange", add = TRUE)
plot(roc.perf3,col="blue", add = TRUE)
plot(perfl,col="yellow", add = TRUE)
legend("bottomright",legend=c("Forward","Stepwise","Backward", "Lasso"),col=c("black","orange","blue","yellow"),lty=1,lwd=1)
abline(a=0, b= 1)
```


# Lasso coeficient
```{r }
lassoc<-glm(y ~ job + education + marital + housing + contact + default + month + day_of_week + duration + campaign + pdays + poutcome + emp.var.rate + cons.conf.idx + cons.price.idx + nr.employed, family = binomial(link = "logit"),data = balanced_train_bank_20)
lassoc

```



# Objective 2

```{r }
alt_pdays_cat_train = ifelse(balanced_train_bank_20$pdays == 999, 0, 1)
alt_clean_bank_20<- balanced_train_bank_20
alt_clean_bank_20$pdays_cat = alt_pdays_cat_train

alt_pdays_cat_test = ifelse(test_bank_20$pdays == 999, 0, 1) 
alt_clean_bank_20_test<- test_bank_20
alt_clean_bank_20_test$pdays_cat = alt_pdays_cat_test

compx<-logr<- glm(y ~ job + default + contact + month + duration + campaign + pdays_cat +poutcome +  job*default + contact*duration + pdays_cat*contact + pdays*duration, family = binomial(link = "logit"),data = alt_clean_bank_20)

```


# prediction for complex model
```{r }
compx<- glm(y ~ job + default + contact + month + duration + campaign + pdays_cat +poutcome +  job*default + contact*duration + pdays_cat*contact + pdays*duration, family = binomial(link = "logit"),data = alt_clean_bank_20)

# 1 way
logr.probs<-predict(compx, newdata=alt_clean_bank_20_test)
logr.pred<-rep("No",32372)
logr.pred[logr.probs>.5]="Yes"
Truth<-test_bank_20[,21]
Pred<-logr.pred
ftable(addmargins(table(Pred,Truth)))
x<-table(Pred,Truth)
ME<-(x[1,2]+x[2,1])/sum(x)
FP<-x[2,1]/(x[2,1]+x[1,1])
FN<-x[1,2]/(x[1,2]+x[2,2])
ses<-x[2,2]/(x[2,2]+x[1,2])
sp<-x[1,1]/(x[1,1]+x[2,1])
accuracy <- x
AC<- sum(diag(accuracy))/sum(accuracy)
TP= 1-FN
AC
TP
ME
FP
FN
ses
sp
# 2 way
#pred = predict(logr, newdata=test_bank_20)
#accuracy <- table(pred, test_bank_20[,21])
#sum(diag(accuracy))/sum(accuracy)
#confusionMatrix(factor(pred, levels = 1:928), factor(balanced_test$y, levels = 1:928))

```


# ROC curve for the complicated model model
```{r }
dat.train.x <- as.data.frame(balanced_train_bank_20)
dat.train.y <- balanced_train_bank_20$y
dat.train.y <- as.factor(as.character(dat.train.y))
dat.test.x<-as.data.frame(test_bank_20)
dat.test.y<- test_bank_20$y
dat.test.y <- as.factor(as.character(dat.test.y))

compx<- glm(y ~ job + default + contact + month + duration + campaign + pdays +poutcome +  job*default + contact*duration + pdays*contact + pdays*duration, family = binomial(link = "logit"),data = balanced_train_bank_20)

logr.probsc<-predict(compx, newdata=dat.test.x)

#Compare the prediction to the real outcome
head(logr.probsc)
head(dat.train.y)

#Create ROC curves
predc <- prediction(logr.probsc, dat.test.y)
roc.perfc = performance(predc, measure = "tpr", x.measure = "fpr")
auc.trainc <- performance(predc, measure = "auc")
auc.trainc <- auc.trainc@y.values

#Plot ROC
plot(roc.perfc)
abline(a=0, b= 1) #Ref line indicating poor performance
text(x = .40, y = .6,paste("AUC = ", round(auc.trainc[[1]],3), sep = ""))

```


# Compare ROC for competing model model
```{r }
plot( roc.perfc, colorize = TRUE)
plot(roc.perf2, add = TRUE, colorize = TRUE)
plot(roc.perfld, add = TRUE, colorize = TRUE)
abline(a=0, b= 1)

#without color for cutoff; but adding colors to allow for comarisons of the curves
plot( roc.perfc, col="black", add = TRUE)
plot(roc.perf2,col="orange", add = TRUE)
plot(roc.perfld,col="blue", add = TRUE)
legend("bottomright",legend=c("Complicated","Stepwise","LDA")
       ,col=c("orange","blue","yellow"),lty=1,lwd=1)
abline(a=0, b= 1)
```



